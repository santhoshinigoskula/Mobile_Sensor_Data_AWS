{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x34dCn9DF1hL",
        "outputId": "6dffe386-865f-46f6-804f-fd432d8aea5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in /opt/miniconda3/lib/python3.13/site-packages (1.41.5)\r\n",
            "Requirement already satisfied: botocore<1.42.0,>=1.41.5 in /opt/miniconda3/lib/python3.13/site-packages (from boto3) (1.41.5)\r\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/miniconda3/lib/python3.13/site-packages (from boto3) (1.0.1)\r\n",
            "Requirement already satisfied: s3transfer<0.16.0,>=0.15.0 in /opt/miniconda3/lib/python3.13/site-packages (from boto3) (0.15.0)\r\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/miniconda3/lib/python3.13/site-packages (from botocore<1.42.0,>=1.41.5->boto3) (2.9.0.post0)\r\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/miniconda3/lib/python3.13/site-packages (from botocore<1.42.0,>=1.41.5->boto3) (2.5.0)\r\n",
            "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.42.0,>=1.41.5->boto3) (1.17.0)\r\n"
          ]
        }
      ],
      "source": [
        "! pip install boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odd0wCbaF1hM"
      },
      "source": [
        "# enter values and run this Cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1pDhOfFF1hN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import boto3\n",
        "import shutil\n",
        "import glob\n",
        "import datetime as dt\n",
        "from datetime import date, datetime, timedelta\n",
        "\n",
        "\n",
        "bucket_name = 'sensus3-91a4f63b-9cc4-4213-b64c-8cbe72a52dc7'\n",
        "\n",
        "enter_aws_access_key_id ='*******************'\n",
        "\n",
        "enter_aws_secret_access_key ='********************'\n",
        "\n",
        "#NO SPACE between group & number\n",
        "group_number = 'group8'\n",
        "\n",
        "# path to the folder to save NO SAPCE in folder name\n",
        "save_folder_path = '/Users/santhoshinigoskula/Documents'\n",
        "\n",
        "# the folder names in the data folder your group members names as apear on AWS S3\n",
        "#for example\n",
        "participent_list = ['Proj_0688','Proj_0905','proj_0914','proj_0922','proj_1068','proj_4830','proj_6815']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7Z0wnNDF1hN"
      },
      "source": [
        "# Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ujDzf1d8F1hN",
        "outputId": "5708523e-c3ad-4cb6-a76f-fbc2b1c713a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Participants to download: ['Proj_0688', 'Proj_0905', 'Proj_0922', 'proj_0914', 'proj_1068', 'proj_4830', 'proj_6815']\n",
            "â¬‡ï¸  Starting download for prefix: data/20251122/Proj_0688\n",
            "\n",
            "ðŸ”Ž Checking prefix: data/20251122/Proj_0688\n",
            "â¬‡ï¸  Downloaded: data/20251122/Proj_0688/04487ac7-c29f-48c9-89da-d2c660f98735.json.gz\n",
            "â¬‡ï¸  Downloaded: data/20251122/Proj_0688/182aa276-839f-4ffc-b93c-5c1d97341f79.json.gz\n",
            "â¬‡ï¸  Downloaded: data/20251122/Proj_0688/f0561c70-07cb-4041-8a60-468059962bdf.json.gz\n",
            "â¬‡ï¸  Starting download for prefix: data/20251122/Proj_0905\n",
            "\n",
            "ðŸ”Ž Checking prefix: data/20251122/Proj_0905\n",
            "â¬‡ï¸  Downloaded: data/20251122/Proj_0905/808a1e25-1c37-40b4-805f-4c8da3e389b6.json.gz\n",
            "â¬‡ï¸  Starting download for prefix: data/20251122/Proj_0922\n",
            "\n",
            "ðŸ”Ž Checking prefix: data/20251122/Proj_0922\n",
            "â¬‡ï¸  Downloaded: data/20251122/Proj_0922/6c34ff7d-ccd5-4ebb-953e-8c349e91ab42.json.gz\n",
            "â¬‡ï¸  Starting download for prefix: data/20251122/proj_0914\n",
            "\n",
            "ðŸ”Ž Checking prefix: data/20251122/proj_0914\n",
            "â¬‡ï¸  Downloaded: data/20251122/proj_0914/6b43406f-2041-4e62-8bfc-cef74b77def4.json.gz\n",
            "â¬‡ï¸  Downloaded: data/20251122/proj_0914/98f6c7ec-f97e-4e71-aeca-292d868273b8.json.gz\n",
            "â¬‡ï¸  Downloaded: data/20251122/proj_0914/e119c21e-4547-4e14-a720-7410adf2a7e1.json.gz\n",
            "â¬‡ï¸  Downloaded: data/20251122/proj_0914/fe13dfcd-07e4-4fc6-9bb3-b045fd61b774.json.gz\n",
            "â¬‡ï¸  Starting download for prefix: data/20251122/proj_1068\n",
            "\n",
            "ðŸ”Ž Checking prefix: data/20251122/proj_1068\n",
            "â¬‡ï¸  Downloaded: data/20251122/proj_1068/1a335d65-ca1e-4fb0-99a3-06751f1f3e0c.json.gz\n",
            "â¬‡ï¸  Downloaded: data/20251122/proj_1068/e89f90bd-23f6-4087-b999-6f54273a974c.json.gz\n",
            "â¬‡ï¸  Starting download for prefix: data/20251122/proj_4830\n",
            "\n",
            "ðŸ”Ž Checking prefix: data/20251122/proj_4830\n",
            "â¬‡ï¸  Downloaded: data/20251122/proj_4830/a9a3c09a-2901-42b8-af22-ac805afe124d.json.gz\n",
            "â¬‡ï¸  Starting download for prefix: data/20251122/proj_6815\n",
            "\n",
            "ðŸ”Ž Checking prefix: data/20251122/proj_6815\n",
            "â¬‡ï¸  Downloaded: data/20251122/proj_6815/b374d206-e321-4798-8d6f-e7c56ffa784a.json.gz\n",
            "â¬‡ï¸  Downloaded: data/20251122/proj_6815/d3a5f563-285e-4da0-9985-3ee5669ec37a-incomplete.json.gz\n"
          ]
        }
      ],
      "source": [
        "s3 = boto3.resource('s3',\n",
        "                 aws_access_key_id= enter_aws_access_key_id,\n",
        "                 aws_secret_access_key= enter_aws_secret_access_key)\n",
        "\n",
        "\n",
        "#     ## Bucket to use\n",
        "bucket = s3.Bucket(bucket_name)\n",
        "\n",
        "\n",
        "#get participant list\n",
        "def get_participant_list(bucket_input):\n",
        "    client = boto3.client('s3',\n",
        "                     aws_access_key_id= enter_aws_access_key_id,\n",
        "                     aws_secret_access_key= enter_aws_secret_access_key)\n",
        "    result = client.list_objects(Bucket=bucket_input, Prefix='data/', Delimiter='/')\n",
        "    part_list = []\n",
        "    for o in result.get('CommonPrefixes'):\n",
        "        part_list.append(o.get('Prefix').split('/')[1])\n",
        "    return part_list\n",
        "\n",
        "#update this to teh correct format\n",
        "\n",
        "\n",
        "#path of the files\n",
        "file_list=[]\n",
        "\n",
        "\n",
        "# def get_objects(part_name):\n",
        "#     for obj in bucket.objects.filter(Delimiter='/', Prefix=f'data/{part_name}/'):\n",
        "#         file_list.append(obj.key)\n",
        "\n",
        "# for i in range(len(participent_list)):\n",
        "#     get_objects(participent_list[i])\n",
        "def download_dir(prefix, local, bucket):\n",
        "    client = boto3.client(\n",
        "        \"s3\",\n",
        "        region_name='us-east-1',\n",
        "        aws_access_key_id=enter_aws_access_key_id,\n",
        "        aws_secret_access_key=enter_aws_secret_access_key\n",
        "    )\n",
        "\n",
        "    print(f\"\\nðŸ”Ž Checking prefix: {prefix}\")\n",
        "    keys = []\n",
        "    dirs = []\n",
        "    next_token = \"\"\n",
        "    base_kwargs = {\n",
        "        \"Bucket\": bucket,\n",
        "        \"Prefix\": prefix,\n",
        "    }\n",
        "\n",
        "    while next_token is not None:\n",
        "        kwargs = base_kwargs.copy()\n",
        "        if next_token != \"\":\n",
        "            kwargs.update({\"ContinuationToken\": next_token})\n",
        "\n",
        "        results = client.list_objects_v2(**kwargs)\n",
        "        contents = results.get(\"Contents\")\n",
        "\n",
        "        # âœ… If S3 found nothing under this prefix, stop gracefully\n",
        "        if not contents:\n",
        "            print(f\"âš ï¸  No objects found for prefix: {prefix}\")\n",
        "            return\n",
        "\n",
        "        for item in contents:\n",
        "            k = item.get(\"Key\")\n",
        "            if k.endswith(\"/\"):\n",
        "                dirs.append(k)\n",
        "            else:\n",
        "                keys.append(k)\n",
        "\n",
        "        next_token = results.get(\"NextContinuationToken\")\n",
        "\n",
        "    # Create local directories\n",
        "    for d in dirs:\n",
        "        dest_pathname = os.path.join(local, d)\n",
        "        os.makedirs(os.path.dirname(dest_pathname), exist_ok=True)\n",
        "\n",
        "    # Download files\n",
        "    for k in keys:\n",
        "        dest_pathname = os.path.join(local, k)\n",
        "        os.makedirs(os.path.dirname(dest_pathname), exist_ok=True)\n",
        "        client.download_file(bucket, k, dest_pathname)\n",
        "        print(f\"â¬‡ï¸  Downloaded: {k}\")\n",
        "\n",
        "\n",
        "date_folder = \"20251122\"   # from your S3 keys\n",
        "\n",
        "participent_list = [\n",
        "    \"Proj_0688\",\n",
        "    \"Proj_0905\",\n",
        "    \"Proj_0922\",\n",
        "    \"proj_0914\",\n",
        "    \"proj_1068\",\n",
        "    \"proj_4830\",\n",
        "    \"proj_6815\",\n",
        "]\n",
        "\n",
        "print(\"Participants to download:\", participent_list)\n",
        "\n",
        "for pid in participent_list:\n",
        "    prefix = f\"data/{date_folder}/{pid}\"\n",
        "    print(\"â¬‡ï¸  Starting download for prefix:\", prefix)\n",
        "    download_dir(prefix, save_folder_path, bucket_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgFnPn6DF1hO"
      },
      "source": [
        "# Creating the data set and output CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B2Wk-nKF1hO",
        "outputId": "2586eb90-5c62-4de2-a6c4-916c2c19dffe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Copied files from /Users/santhoshinigoskula/Documents/data/20251122/Proj_0688 to /Users/santhoshinigoskula/Documents/data/allfiles\n",
            "âœ… Copied files from /Users/santhoshinigoskula/Documents/data/20251122/Proj_0905 to /Users/santhoshinigoskula/Documents/data/allfiles\n",
            "âœ… Copied files from /Users/santhoshinigoskula/Documents/data/20251122/Proj_0922 to /Users/santhoshinigoskula/Documents/data/allfiles\n",
            "âœ… Copied files from /Users/santhoshinigoskula/Documents/data/20251122/proj_0914 to /Users/santhoshinigoskula/Documents/data/allfiles\n",
            "âœ… Copied files from /Users/santhoshinigoskula/Documents/data/20251122/proj_1068 to /Users/santhoshinigoskula/Documents/data/allfiles\n",
            "âœ… Copied files from /Users/santhoshinigoskula/Documents/data/20251122/proj_4830 to /Users/santhoshinigoskula/Documents/data/allfiles\n",
            "âœ… Copied files from /Users/santhoshinigoskula/Documents/data/20251122/proj_6815 to /Users/santhoshinigoskula/Documents/data/allfiles\n"
          ]
        }
      ],
      "source": [
        "#Creating the dataset function\n",
        "def create_combined_dataset(path_to_raw_data_folder):\n",
        "    data = [pd.read_json(f'{path_to_raw_data_folder}/{i}') for i in os.listdir(path_to_raw_data_folder) if i.endswith('.gz')]\n",
        "    return pd.concat(data)\n",
        "\n",
        "# Function to fix timestamp from UTC to EST\n",
        "def fix_time(a):\n",
        "    # If timestamp is missing or empty, return NaT\n",
        "    if pd.isna(a) or a == \"\":\n",
        "        return pd.NaT\n",
        "\n",
        "    # Try to parse and convert; if it fails, return NaT\n",
        "    try:\n",
        "        b = pd.to_datetime(a, utc=True)\n",
        "        if pd.isna(b):\n",
        "            return pd.NaT\n",
        "\n",
        "        # Convert UTC â†’ America/New_York\n",
        "        c = b.tz_convert(\"America/New_York\")\n",
        "        # Return formatted string (or keep as datetime if you prefer)\n",
        "        return c.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    except Exception:\n",
        "        return pd.NaT\n",
        "\n",
        "\n",
        "try:\n",
        "    os.mkdir(save_folder_path+'/data/allfiles')\n",
        "except:\n",
        "    pass\n",
        "#Move all the results into a single folder\n",
        "for j in range(len(participent_list)):\n",
        "    date_folder = \"20251122\"  # same one you used earlier\n",
        "\n",
        "for j in range(len(participent_list)):\n",
        "    pid = participent_list[j]\n",
        "\n",
        "    source_dir = os.path.join(save_folder_path, \"data\", date_folder, pid)\n",
        "    target_dir = os.path.join(save_folder_path, \"data\", \"allfiles\")\n",
        "\n",
        "    # Make sure target_dir exists\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(source_dir):\n",
        "        print(f\"âš ï¸ Source folder does not exist, skipping: {source_dir}\")\n",
        "        continue\n",
        "\n",
        "    file_names = os.listdir(source_dir)\n",
        "\n",
        "    for file_name in file_names:\n",
        "        shutil.copy(os.path.join(source_dir, file_name), target_dir)\n",
        "\n",
        "    print(f\"âœ… Copied files from {source_dir} to {target_dir}\")\n",
        "\n",
        "# find corrupt files, move them to a corrupt folder\n",
        "zip_file_names = list(glob.glob(target_dir+\"/*.gz\"))\n",
        "corrupt_files = []\n",
        "ok_files_count=0\n",
        "corrupt_file_count = 0\n",
        "for i in range(len(zip_file_names)):\n",
        "    try:\n",
        "        pd.read_json(zip_file_names[i])\n",
        "        ok_files_count += 1\n",
        "        #print(zip_file_names[i] +' ---> FILE OK')\n",
        "    except:\n",
        "        corrupt_files.append(zip_file_names[i])\n",
        "        #print(zip_file_names[i]+' ---> CORRUPTED')\n",
        "        corrupt_file_count += 1\n",
        "\n",
        "################################ folders for files\n",
        "try:\n",
        "    os.mkdir(save_folder_path+'/data/corrupt_file_folder')\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    for i in range(len(corrupt_files)):\n",
        "        shutil.move(f\"{corrupt_files[i]}\", f\"{save_folder_path}/data/corrupt_file_folder\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "df = create_combined_dataset(target_dir)\n",
        "\n",
        "df.rename(columns={'$type':'Type'},inplace=True) #fixing the name\n",
        "\n",
        "df.rename(columns={'Level':'BatteryLevel'},inplace=True) #fixing the level name\n",
        "\n",
        "df.rename(columns={'Accuracy':'LocationAccuracy'},inplace=True) #fixing the accuracy name\n",
        "\n",
        "# this is a metadata column that can remove\n",
        "try:\n",
        "    df.drop('ProbeParticipation', axis=1, inplace=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "# If the column protocol name has the protocol name we remove it since every\n",
        "# participent is in the same study\n",
        "try:\n",
        "    df= df[df['ProtocolName'].isnull()] #removing info row\n",
        "except:\n",
        "    pass\n",
        "\n",
        "#\n",
        "df.index = pd.RangeIndex(len(df.index)) #fixing the index\n",
        "df.index.name = 'Row_id' #adding a new id row\n",
        "df['Formatted_time'] = df['Timestamp'].apply(lambda x: fix_time(x)) # adding a new column changing UTC to EST\n",
        "df['Formatted_time'] = pd.to_datetime(df['Formatted_time'] ) # fix data type\n",
        "\n",
        "\n",
        "# remove extra\n",
        "df['Type'] = df['Type'].replace(regex = {'Sensus.Participation':''})\n",
        "df['Type'] = df['Type'].replace(regex = {' SensusiOS':''})\n",
        "df['Type'] = df['Type'].replace(regex = {'Sensus.Probes.Device.': ''})\n",
        "df['Type'] = df['Type'].replace(regex = {'Sensus.Probes.Location.': ''})\n",
        "df['Type'] = df['Type'].replace(regex = {'Sensus.Probes.Movement.':''})\n",
        "df['Type'] = df['Type'].replace(regex = {'Sensus.Probes.Network.': ''})\n",
        "df['Type'] = df['Type'].replace(regex = {'Sensus.Probes.Context.': ''})\n",
        "df['Type'] = df['Type'].replace(regex = {'Datum,':''})\n",
        "\n",
        "# removing metadata rows\n",
        "df = df[df['Type'] != 'Sensus.Heartbeat']\n",
        "df = df[df['Type'] != 'Report']\n",
        "\n",
        "### fixing the values for mysql\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'],utc=True)\n",
        "df['Formatted_time'] = pd.to_datetime(df['Formatted_time'] )\n",
        "\n",
        "    # fixing the order\n",
        "try:\n",
        "    df = df[['Type','Id','DeviceId','Timestamp','ProtocolId','BuildId','ParticipantId','DeviceManufacturer',\n",
        "         'DeviceModel','OperatingSystem','TaggedEventId','TaggedEventTags','SensingAgentStateDescription',\n",
        "         'LocalOffsetFromUTC','Decibels','BatteryLevel','AccessPointBSSID','Activity','Phase','State',\n",
        "         'Confidence','Latitude','Longitude','LocationAccuracy','ProtocolName','Formatted_time']]\n",
        "except:\n",
        "    df = df[['Type','Id','DeviceId','Timestamp','ProtocolId','BuildId','ParticipantId','DeviceManufacturer',\n",
        "         'DeviceModel','OperatingSystem','TaggedEventId','TaggedEventTags','SensingAgentStateDescription',\n",
        "         'LocalOffsetFromUTC','Decibels','BatteryLevel','AccessPointBSSID','Activity','Phase','State',\n",
        "         'Confidence','ProtocolName','Formatted_time']]\n",
        "\n",
        "df.to_csv(f'{save_folder_path}/{group_number}_output.csv')\n",
        "\n",
        "shutil.rmtree(f'{save_folder_path}/data/allfiles')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z7RcVJwF1hO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Project2 Env",
      "language": "python",
      "name": "p2env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
